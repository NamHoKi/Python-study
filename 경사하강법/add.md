##경사하강법
### 기본적인 경사하강법
##### 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있다.
##### 특히 선형회귀의 경우 L2 노름에선 충분히 돌리면 수렴 보장
##### 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않음
-----------------------
### 확률적 경사하강법(stochastic gradient descent,SGD)
##### 모든 데이터를 사용하는 것이 아님 -> 데이터 하나 or 데이터의 일부
##### 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있음
##### 데이터의 일부를 사용해서 업데이트하므로 연산자원을 효율적으로 활용 가능
##### 미니배치  사이즈를 고려해야함
